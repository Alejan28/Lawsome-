{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Modelul este prea mare, executia nu a ajuns pana la final"
      ],
      "metadata": {
        "id": "hAlh35iAHUc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWF1F9xoB7bS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "MODEL_NAME = \"OpenLLM-Ro/RoLlama2-7b-Instruct-DPO-2025-04-23\"\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Model încărcat:\", MODEL_NAME)\n",
        "print(\"GPU disponibil:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Mulțumesc — rulezi pe GPU!\", torch.cuda.get_device_name(0))\n",
        "\n",
        "\n",
        "SYSTEM_MSG = (\n",
        "    \"Ești un jurist care redactează cereri judiciare clare, concise şi conforme codului de procedură civilă din România. \"\n",
        "    \"Creează text complet în limba română, folosind formule standard juridice precum „Subsemnatul…”, „Pentru aceste motive…”, „Vă rugăm să…”.\"\n",
        ")\n",
        "\n",
        "TEMPLATE = \"\"\"\n",
        "Redactează o {tip_cerere} completă în limba română, pe baza următoarelor date:\n",
        "- Instanță: {instanta}\n",
        "- Reclamant / Apelant / Recursant: {reclamant}\n",
        "- Pârât / Intimat: {parat}\n",
        "- Obiectul cererii: {obiect}\n",
        "- Temei legal: {temei}\n",
        "- Probe: {probe}\n",
        "- Ton: formal și juridic\n",
        "- Lungime dorită: {lungime} cuvinte\n",
        "\"\"\"\n",
        "\n",
        "tipuri = [\"cerere de chemare în judecată\", \"cerere de apel\", \"cerere de recurs\"]\n",
        "instante = [\"Judecătoria București\", \"Tribunalul Cluj\", \"Curtea de Apel Iași\"]\n",
        "reclamanti = [\"subsemnatul A.B.\", \"apelantul C.D.\", \"recursantul E.F.\"]\n",
        "parati = [\"Societatea X S.R.L.\", \"pârâtul G.H.\", \"intimatul I.J.\"]\n",
        "obiecte = [\n",
        "    \"obligarea pârâtului la plata sumei de 10.000 lei reprezentând daune morale\",\n",
        "    \"anularea hotărârii civile nr. 123/2024 a Tribunalului Cluj\",\n",
        "    \"modificarea dispozitivului sentinței și respingerea acțiunii ca neîntemeiată\"\n",
        "]\n",
        "temeiuri = [\n",
        "    \"art. 1349 Cod civil și art. 194 C.proc.civ.\",\n",
        "    \"art. 466–480 C.proc.civ.\",\n",
        "    \"art. 488 alin. 1 pct. 8 C.proc.civ.\"\n",
        "]\n",
        "probe = [\n",
        "    \"înscrisuri, martori, înregistrări video\",\n",
        "    \"contractul anexat, corespondența electronică, facturi\",\n",
        "    \"hotărâri anterioare și dovezi de plată\"\n",
        "]\n",
        "lungimi = [150, 200, 250]\n",
        "\n",
        "def sample_prompt():\n",
        "    return TEMPLATE.format(\n",
        "        tip_cerere=random.choice(tipuri),\n",
        "        instanta=random.choice(instante),\n",
        "        reclamant=random.choice(reclamanti),\n",
        "        parat=random.choice(parati),\n",
        "        obiect=random.choice(obiecte),\n",
        "        temei=random.choice(temeiuri),\n",
        "        probe=random.choice(probe),\n",
        "        lungime=random.choice(lungimi)\n",
        "    )\n",
        "\n",
        "def call_model(system, prompt, temperature=0.3, max_new_tokens=400):\n",
        "    full_prompt = f\"System: {system}\\nUser: {prompt}\\nAssistant:\"\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "    p = sample_prompt()\n",
        "    print(f\"\\n=== Prompt {i+1} ===\\n{p}\")\n",
        "    out = call_model(SYSTEM_MSG, p)\n",
        "    print(f\"\\n--- Cerere generată ---\\n{out}\\n\")\n"
      ]
    }
  ]
}